{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# to display full text\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'graph_eligible_data_v2.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice = df[['src_user_id', 'src_text','reply_user_id', 'reply_user_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt\n",
    "\n",
    "df['clean_text_initiator'] = np.vectorize(remove_pattern)(df['src_text'], \"@[\\w]*\")\n",
    "df['clean_text_reply'] = np.vectorize(remove_pattern)(df['reply_user_text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text_initiator'] = df['clean_text_initiator'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "df['clean_text_reply'] = df['clean_text_reply'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text_initiator'] = df.clean_text_initiator.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "df['clean_text_reply'] = df.clean_text_reply.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean_text_initiator = df.clean_text_initiator.apply(lambda x: x.split())\n",
    "df.clean_text_reply = df.clean_text_reply.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet_text = df.clean_text_initiator.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet_text1 = df.clean_text_reply.apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet_text)):\n",
    "    tokenized_tweet_text[i] = ' '.join(tokenized_tweet_text[i])    \n",
    "df['clean_text_initiator'] = tokenized_tweet_text\n",
    "\n",
    "for i in range(len(tokenized_tweet_text1)):\n",
    "    tokenized_tweet_text1[i] = ' '.join(tokenized_tweet_text1[i])    \n",
    "df['clean_text_reply'] = tokenized_tweet_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_verified_user'] = df['src_verified_user'].apply(lambda x: 1 if \"True\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reply_verified_user'] = df['reply_verified_user'].apply(lambda x: 1 if \"True\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['reply_label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antonym and jaccard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_score(str1, str2): \n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshold = 0.05\n",
    "c = 1\n",
    "for i in range (0, len(df)):\n",
    "    print(i)\n",
    "    initiator_label = df.iloc[i]['label']\n",
    "    antonyms = []\n",
    "    initiator_token = []\n",
    "    reply_token = []\n",
    "    antonyms_count = 0\n",
    "    line_initiator = df.iloc[i]['clean_text_initiator']\n",
    "    line_reply = df.iloc[i]['clean_text_reply']\n",
    "    \n",
    "    #tokens for initiators sentences\n",
    "    #for l in line_initiator:\n",
    "    initiator_token = word_tokenize(line_initiator)\n",
    "    if (c == 1):\n",
    "        print(initiator_token)\n",
    "        c = c + 1\n",
    "    \n",
    "    #tokens for reply sentences\n",
    "    #for l1 in line_reply:\n",
    "    reply_token = word_tokenize(line_reply)\n",
    "    \n",
    "    #generate antonyms for initiator sentence\n",
    "    for token in initiator_token:\n",
    "        for syn in wordnet.synsets(token):\n",
    "            for l2 in syn.lemmas():\n",
    "                if l2.antonyms():\n",
    "                    antonyms.append(l2.antonyms()[0].name())\n",
    "    \n",
    "    #comparing antonyms of initiator sentence with reply sentence\n",
    "    for word in antonyms:\n",
    "        for compared_word in reply_token:\n",
    "            if (word == compared_word):\n",
    "                print(\"word \",word, \" compared \", compared_word)\n",
    "                antonyms_count = antonyms_count + 1\n",
    "            \n",
    "    #score calculation\n",
    "    #antonym_score = (antonyms_count) / (len(initiator_token) + len(reply_token))\n",
    "    #print(antonym_score)\n",
    "    \n",
    "    jaccard_score = jaccard_similarity_score(df.iloc[i]['clean_text_initiator'], df.iloc[i]['clean_text_reply'])\n",
    "    \n",
    "    count = 0\n",
    "    #assigning labels to reply nodes\n",
    "    if (antonyms_count >= 1 or jaccard_score < 0.5):\n",
    "        print(count)\n",
    "        count = count + 1\n",
    "        df.loc[i,'reply_label'] = 1 - initiator_label\n",
    "        #print(\" reply label for opposites: \", df.loc[i,'reply_label'])\n",
    "        #print(\"if:\", df.iloc[i]['label'], \" reply: \", )\n",
    "    else:\n",
    "        df.loc[i,'reply_label'] = initiator_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reply_label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in range(0, len(df)):\n",
    "    if (df.iloc[i]['reply_label'] == 0):\n",
    "        count_0 = count_0 + 1\n",
    "    elif (df.iloc[i]['reply_label'] == 1):\n",
    "        count_1 = count_1 + 1\n",
    "print(\"0: \", count_0)\n",
    "print(\"1: \", count_1)\n",
    "percent_0 = (count_0) / (len(df) ) * 100\n",
    "print(percent_0)\n",
    "\n",
    "percent_1 = (count_1) / (len(df) ) * 100\n",
    "print(percent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['reply_label'].apply(lambda x: x if 1 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('charlie_replynode_labelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('charlie_replynode_labelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['src_user_id'] == df['reply_user_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18992 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    indices.append(i)\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user_id', 'text', 'favorites_count', 'verified', 'followers_count', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.DataFrame(data = np.zeros((df.shape[0], 6)), index = indices, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_initiators = df['src_user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding data in linear fashion asll user ids in one column for src_ids here\n",
    "for i in range (0, len(list_of_initiators)):\n",
    "    print(i)\n",
    "    count = 0\n",
    "    for j in range(0, len(df)):\n",
    "        if ((list_of_initiators[i] == df.iloc[j]['src_user_id']) & (count == 0)):\n",
    "            count = count + 1\n",
    "            df_graph.loc[i,'user_id'] = df.iloc[j]['src_user_id']\n",
    "            df_graph.loc[i,'text'] = df.iloc[j]['clean_text_initiator']\n",
    "            df_graph.loc[i,'favorites_count'] = df.iloc[j]['src_favorites_count']\n",
    "            df_graph.loc[i,'followers_count'] = df.iloc[j]['src_followers_count']\n",
    "            df_graph.loc[i,'verified'] = df.iloc[j]['src_verified_user']\n",
    "            df_graph.loc[i,'label'] = df.iloc[j]['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.iloc[1005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_replies = df['reply_user_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.to_csv('graph-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.read_csv('graph-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = df_graph.drop('Unnamed: 0' , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.iloc[1004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iloc[0]['clean_text_reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding data in linear fashion as all user ids in one column for reply ids here\n",
    "k = len(list_of_initiators)\n",
    "for i in range (0, len(list_of_replies)):\n",
    "    k = k + 1\n",
    "    print(i)\n",
    "    count = 0\n",
    "    text = ''\n",
    "    for j in range(0, len(df)):\n",
    "        if (list_of_replies[i] == df.iloc[j]['reply_user_id']):\n",
    "            if (count == 0):\n",
    "                df_graph.loc[k,'user_id'] = df.iloc[j]['reply_user_id']\n",
    "                df_graph.loc[k,'favorites_count'] = df.iloc[j]['reply_favorites_count']\n",
    "                df_graph.loc[k,'followers_count'] = df.iloc[j]['reply_followers_count']\n",
    "                df_graph.loc[k,'verified'] = df.iloc[j]['reply_verified_user']\n",
    "                df_graph.loc[k,'label'] = df.iloc[j]['reply_label']\n",
    "                count = count + 1\n",
    "                    \n",
    "            text = text + str(df.iloc[j]['clean_text_reply'])\n",
    "\n",
    "    df_graph.loc[k,'text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#adding data in linear fashion asll user ids in one column for reply ids here\n",
    "k = 17991\n",
    "for i in range (0, 1006):\n",
    "    k = k + 1\n",
    "    print(i)\n",
    "    count = 0\n",
    "    text = ''\n",
    "    for j in range(0, len(df)):\n",
    "        if (list_of_replies[i] == df.iloc[j]['reply_user_id']):\n",
    "            if (count == 0):\n",
    "                df_graph.loc[k,'user_id'] = df.iloc[j]['reply_user_id']\n",
    "                df_graph.loc[k,'favorites_count'] = df.iloc[j]['reply_favorites_count']\n",
    "                df_graph.loc[k,'followers_count'] = df.iloc[j]['reply_followers_count']\n",
    "                df_graph.loc[k,'verified'] = df.iloc[j]['reply_verified_user']\n",
    "                df_graph.loc[k,'label'] = df.iloc[j]['reply_label']\n",
    "                count = count + 1\n",
    "                    \n",
    "            text = text + str(df.iloc[j]['clean_text_reply'])\n",
    "\n",
    "    df_graph.loc[k,'text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_graph.to_csv('graph-datav2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_graph.to_csv('graph-datav3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels to user suspicious score (for src_ids)\n",
    "for i in range (0, len(list_of_initiators)):\n",
    "    print(i)\n",
    "    label_0 = 0\n",
    "    label_1 = 0\n",
    "    for j in range(0, len(df)):\n",
    "            if (list_of_initiators[i] == df.iloc[j]['src_user_id']):\n",
    "                if (df.iloc[j]['label'] == 0):\n",
    "                    label_0 = label_0 + 1\n",
    "                elif (df.iloc[j]['label'] == 1):\n",
    "                    label_1 = label_1 + 1\n",
    "    \n",
    "    suspicious_user_intensity = round(((label_1)/(label_1 + label_0)), 2)            \n",
    "    print(\"suspicious_user_intensity: \", suspicious_user_intensity)\n",
    "    df_graph.loc[i,'label'] = suspicious_user_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.iloc[553]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_initiators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels to user suspicious score (for reply_ids)\n",
    "k = -1\n",
    "for i in range (len(list_of_initiators), len(df_graph)):\n",
    "    k = k + 1\n",
    "    print(i)\n",
    "    label_0 = 0\n",
    "    label_1 = 0\n",
    "    for j in range(0, len(df)):\n",
    "            if (list_of_replies[k] == df.iloc[j]['reply_user_id']):\n",
    "                if (df.iloc[j]['reply_label'] == 0):\n",
    "                    label_0 = label_0 + 1\n",
    "                elif (df.iloc[j]['reply_label'] == 1):\n",
    "                    label_1 = label_1 + 1\n",
    "    \n",
    "    suspicious_user_intensity = round(((label_1)/(label_1 + label_0)), 2)            \n",
    "    print(\"suspicious_user_intensity: \", suspicious_user_intensity)\n",
    "    df_graph.loc[i,'label'] = suspicious_user_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_initiators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.iloc[11234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df = df_graph[:11234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## not to be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.to_csv('graph-datav4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df.to_csv('graph-datav4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.read_csv('graph-datav4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in range(0, len(reduce_df)):\n",
    "    if (reduce_df.iloc[i]['label'] == 0):\n",
    "        count_0 = count_0 + 1\n",
    "    elif (reduce_df.iloc[i]['label'] == 1):\n",
    "        count_1 = count_1 + 1\n",
    "print(\"0: \", count_0)\n",
    "print(\"1: \", count_1)\n",
    "percent_0 = (count_0) / (len(reduce_df) ) * 100\n",
    "print(percent_0)\n",
    "\n",
    "percent_1 = (count_1) / (len(reduce_df) ) * 100\n",
    "print(percent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences = text, size = 100, window = 2, workers = 3, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print('Vocab size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('terror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'twitter_charlie_manual_word2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(file,binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "emb = {}\n",
    "f = open(os.path.join('', 'twitter_graph_charlie_manual_word2vec.txt'), encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:])\n",
    "    emb[word] = value\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb['well']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)\n",
    "seq = token.texts_to_sequences(text)\n",
    "index = token.word_index\n",
    "print('unique no of tokens ', index)\n",
    "padding = pad_sequences(seq, maxlen = max_length)\n",
    "rumour = df_graph['label'].values\n",
    "print('Shape of tweet ',padding.shape)\n",
    "print('Shape of rumour ', rumour.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(index) + 1\n",
    "embed_mat = np.zeros((n, 100))\n",
    "for s,values in index.items():\n",
    "    print(s,values)\n",
    "    if values>n:\n",
    "        continue\n",
    "    embed_vec = emb.get(s)\n",
    "    if embed_vec is not None:\n",
    "        embed_mat[values] = embed_vec\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted(index)\n",
    "print(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map words with their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(0, len(padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((0, len(df_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(index = d, data = data, columns = sorted_words)\n",
    "print(feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.iloc[1][1] == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combining features with average\n",
    "for row in range(0, len(padding)):\n",
    "    print(row)\n",
    "    for number in padding[row]:\n",
    "        for key,value in index.items():\n",
    "            if number == value:\n",
    "                mean = embed_mat[number].mean()\n",
    "                if (feat_df.iloc[row][key] == 0.0):\n",
    "                    feat_df.iloc[row][key] = mean\n",
    "                else:\n",
    "                    feat_df.iloc[row][key] = feat_df.iloc[row][key] + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.to_csv('features of word2vec embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df=feat_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Features into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.drop('clean_text', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.concat([feat_df, df_graph], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_new.to_csv('charlie_combined_word2vec_features_with_original_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.Graph()\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    G.add_edge(df.iloc[i]['src_user_id'], df.iloc[i]['reply_user_id'], weight = df.iloc[i]['weight'])\n",
    "\n",
    "nx.degree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.average_clustering(G))\n",
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjacency matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.adjacency_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
