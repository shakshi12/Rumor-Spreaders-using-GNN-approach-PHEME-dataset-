{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path_to_json = r'C:\\Users\\shakshi\\Desktop\\phemerumourschemedataset\\pheme-rumour-scheme-dataset\\threads\\en\\ebola-essien/'\n",
    "\n",
    "folders = [pos_json for pos_json in os.listdir(path_to_json) ]\n",
    "print(folders)  # for me this prints ['foo.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders[0]\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r'C:\\Users\\shakshi\\Desktop\\phemerumourschemedataset\\pheme-rumour-scheme-dataset\\threads\\en\\ebola-essien'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display full text\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from json and write to csv file on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    counter = 0\n",
    "    data = []\n",
    "    within_tweetid = os.path.join(base, folder)\n",
    "    print(within_tweetid)\n",
    "    json_files = [pos for pos in os.listdir(within_tweetid) if pos.endswith('.json')]\n",
    "    #print(json_files)\n",
    "    for jsons in json_files:\n",
    "        if jsons == 'annotation.json':\n",
    "            path = os.path.join(within_tweetid, json_files[0])\n",
    "            with open(path,'r') as f:\n",
    "                annot_json = json.load(f)\n",
    "            annotations_df = pd.DataFrame(annot_json)\n",
    "            #print(annotations_df)\n",
    "            annotations_df_slice = pd.DataFrame(annotations_df['is_rumour'])\n",
    "            annot = annotations_df_slice.iloc[0,:]\n",
    "            annot = annot.reset_index()\n",
    "            annot.drop(\"index\", axis = 1, inplace = True)\n",
    "            s = list(str(annot).split())\n",
    "                    #data = \",\".join(map(str, retweet_df))\n",
    "            for i in s:\n",
    "                data.append(i)\n",
    "                    \n",
    "            #data.append(annotations_df_slice.iloc[0])\n",
    "    \n",
    "        '''elif jsons == 'retweets.json':\n",
    "            \n",
    "            retweets = []\n",
    "            path = os.path.join(within_tweetid, json_files[1])\n",
    "            for line in open(path, 'r'):\n",
    "                retweets.append(json.loads(line))\n",
    "                for retweet in retweets:\n",
    "                    retweet_df = pd.DataFrame(retweet)\n",
    "                    retweet['user']['friends_count']\n",
    "                    retweet_df['verified'] = retweet_df.loc['verified']['user']\n",
    "                    retweet_df['friends_count'] = retweet_df.loc['friends_count']['user']\n",
    "                    retweet_df['favourites_count'] = retweet_df.loc['favourites_count']['user']\n",
    "                    retweet_df['location'] = retweet_df.loc['location']['user']\n",
    "                    retweet_df['followers_count'] = retweet_df.loc['followers_count']['user']\n",
    "                    #retweet_df = retweet_df[['text','id','retweet_count','verified','followers_count','favourites_count','friends_count','location']]\n",
    "                    retweet_df = retweet_df['text']\n",
    "                    retweet_df = retweet_df.reset_index()\n",
    "                    retweet_df.drop(['index'], axis = 1, inplace = True)\n",
    "                    retweet_df = retweet_df.iloc[0,:]\n",
    "                    retweet_df = retweet_df.reset_index()\n",
    "                    retweet_df.drop(\"index\", axis = 1, inplace = True)\n",
    "                    #s = list(str(retweet_df).split())\n",
    "                    #data = \",\".join(map(str, retweet_df))\n",
    "                    #for i in s:\n",
    "                     #   data.append(i)\n",
    "                    data.append(retweet_df)'''\n",
    "                    #print (data)\n",
    "    \n",
    "            \n",
    "    inner_folders = [pos for pos in os.listdir(within_tweetid)]\n",
    "    #print(inner_folders)\n",
    "    for inner in inner_folders:\n",
    "        if inner == 'source-tweets':\n",
    "            \n",
    "            within_source_tweets = os.path.join(within_tweetid, inner)\n",
    "            json_files = [pos for pos in os.listdir(within_source_tweets) if pos.endswith('.json')]\n",
    "            for json_file in json_files:\n",
    "                path = os.path.join(within_source_tweets, json_file)\n",
    "                print(path)\n",
    "                with open(path,'r') as f:\n",
    "                    src_json = json.load(f)\n",
    "                src_df = pd.DataFrame(src_json)\n",
    "                '''src_json['user']['friends_count']\n",
    "                src_df['verified'] = src_df.loc['verified']['user']\n",
    "                src_df['friends_count'] = src_df.loc['friends_count']['user']\n",
    "                src_df['favourites_count'] = src_df.loc['favourites_count']['user']\n",
    "                src_df['location'] = src_df.loc['location']['user']\n",
    "                src_df['followers_count'] = src_df.loc['followers_count']['user']'''\n",
    "                #src_df = src_df[['text','id','retweet_count','verified','followers_count','favourites_count','friends_count','location']]\n",
    "                src_df = src_df['text']\n",
    "                src_df = src_df.reset_index()\n",
    "                src_df.drop(['index'], axis = 1, inplace = True)\n",
    "                src_df = src_df.iloc[0,:]\n",
    "                src_df = src_df.reset_index()\n",
    "                src_df.drop(\"index\", axis = 1, inplace = True)\n",
    "                #s = list(str(src_df).split())\n",
    "                #for i in s:\n",
    "                 #   data.append(i)\n",
    "                    \n",
    "                data.append(src_df.iloc[:1,:])\n",
    "    \n",
    "            #print(json_files)\n",
    "            \n",
    "        '''elif inner == 'reactions':\n",
    "             \n",
    "             within_reactions_tweets = os.path.join(within_tweetid, inner)\n",
    "             json_files = [pos for pos in os.listdir(within_reactions_tweets) if pos.endswith('.json')]\n",
    "             #print(json_files)\n",
    "             for json_file in json_files:\n",
    "                path = os.path.join(within_reactions_tweets, json_file)\n",
    "                with open(path,'r') as f:\n",
    "                    reactions_json = json.load(f)\n",
    "                reactions = pd.DataFrame(reactions_json)\n",
    "                reactions_json['user']['friends_count']\n",
    "                reactions['verified'] = reactions.loc['verified']['user']\n",
    "                reactions['friends_count'] = reactions.loc['friends_count']['user']\n",
    "                reactions['favourites_count'] = reactions.loc['favourites_count']['user']\n",
    "                reactions['location'] = reactions.loc['location']['user']\n",
    "                reactions['followers_count'] = reactions.loc['followers_count']['user']\n",
    "                #reactions = reactions[['text','id','retweet_count','verified','followers_count','favourites_count','friends_count','location']]\n",
    "                reactions = reactions['text']\n",
    "                reactions = reactions.reset_index()\n",
    "                reactions.drop(['index'], axis = 1, inplace = True)\n",
    "                reactions = reactions.iloc[0,:]\n",
    "                reactions = reactions.reset_index()\n",
    "                reactions.drop(\"index\", axis = 1, inplace = True)\n",
    "                #s = list(str(reactions).split())\n",
    "                #for i in s:\n",
    "                 #   data.append(i)\n",
    "                \n",
    "                data.append(reactions.iloc[:1,:])'''\n",
    "    if counter == 0:\n",
    "        with open(r'C:\\Users\\shakshi\\Desktop\\phemerumourschemedataset\\pheme-rumour-scheme-dataset\\threads\\en\\dump_ebola_feature.csv', 'a', encoding = 'utf-8') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow(data)\n",
    "        csvFile.close()\n",
    "    else:\n",
    "        with open(r'C:\\Users\\shakshi\\Desktop\\phemerumourschemedataset\\pheme-rumour-scheme-dataset\\threads\\en\\dump_ebola_feature.csv', 'a', newline = '',encoding = 'utf-8') as csvFile:\n",
    "            writer = csv.writer(csvFile)\n",
    "            writer.writerow(data)\n",
    "        csvFile.close()\n",
    "    counter = counter + 1\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "print(within_tweetid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv file where data is written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\shakshi\\Desktop\\phemerumourschemedataset\\pheme-rumour-scheme-dataset\\threads\\en\\dump_ebola_feature.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[[0,2],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token.fit_on_texts(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(tweet.split()) for tweet in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_token = token.texts_to_sequences(X_train)\n",
    "#X_test_token = token.texts_to_sequences(X_test)\n",
    "#X_train_p = pad_sequences(X_train_token, maxlen = max_len, padding = 'post')\n",
    "#X_test_p = pad_sequences(X_test_token, maxlen = max_len, padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "text = list()\n",
    "line = df['text'].values.tolist()\n",
    "for l in line:\n",
    "    token = word_tokenize(l)\n",
    "    token = [t.lower() for t in token]\n",
    "    punct = str.maketrans('','',string.punctuation)\n",
    "    translate = [t.translate(punct) for t in token]\n",
    "    words = [word for word in translate if word.isalpha()]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [t for t in words if not t in stop]\n",
    "    text.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences = text, size = 100, window = 2, workers = 3, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print('Vocab size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'twitter_rumour_detection.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(file,binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "emb = {}\n",
    "f = open(os.path.join('', 'twitter_rumour_detection.txt'), encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:])\n",
    "    emb[word] = value\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb['well']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)\n",
    "seq = token.texts_to_sequences(text)\n",
    "index = token.word_index\n",
    "print('unique no of tokens ', index)\n",
    "padding = pad_sequences(seq, maxlen = max_len)\n",
    "rumour = df['label'].values\n",
    "print('Shape of tweet ',padding.shape)\n",
    "print('Shape of rumour ', rumour.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(index) + 1\n",
    "embed_mat = np.zeros((n, 100))\n",
    "for s,values in index.items():\n",
    "    print(s,values)\n",
    "    if values>n:\n",
    "        continue\n",
    "    embed_vec = emb.get(s)\n",
    "    if embed_vec is not None:\n",
    "        embed_mat[values] = embed_vec\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted(index)\n",
    "print(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map words with their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(columns = sorted_words)\n",
    "print(feat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in padding[0]:\n",
    "    for key,value in index.items():\n",
    "        if number == value:\n",
    "            feat_df[key] = embed_mat[number]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df=feat_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df2 = pd.DataFrame(columns = sorted_words)\n",
    "for number in padding[1]:\n",
    "    for key,value in index.items():\n",
    "        if number == value:\n",
    "            feat_df2[key] = embed_mat[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df2=feat_df2.fillna(0)\n",
    "feat_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Features into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of 100 dimensions of 1 word\n",
    "feature_1 = feat_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2 = feat_df2.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [feature_1, feature_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix = pd.concat(temp, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2 = pd.DataFrame(data = features_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2 = features_matrix2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2['rumour'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2.iloc[0,-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_matrix2.iloc[:,:-1].values\n",
    "y_train = features_matrix2.iloc[:,-1].values\n",
    "X_test = features_matrix2.iloc[:,:-1].values\n",
    "y_test = features_matrix2.iloc[::,-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "recall_logistic = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Confusion matrix \\n',confusion_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.apply(lambda x: ','.join(x.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.DataFrame({'clean': df2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [row.split() for row in clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=1,size= 50,workers=3, window =3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_closestwords_tsnescatterplot(model, word, size):\n",
    "    \n",
    "    arr = np.empty((0,size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    close_words = model.similar_by_word(word)\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "        plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "        plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "        plt.show()\n",
    "display_closestwords_tsnescatterplot(model, 'claim', 50) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a plot of the projection\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(result[:, 0], result[:, 1], 'o')\n",
    "ax.set_title('Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = TSNE(X, 2, 50, 30.0)\n",
    "Y = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the t-SNE output\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y[:, 0], Y[:, 1], 'o')\n",
    "ax.set_title('Tweets')\n",
    "ax.set_yticklabels([]) #Hide ticks\n",
    "ax.set_xticklabels([]) #Hide ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (16,16))\n",
    "ax.plot(Y[:, 0], Y[:, 1], 'o')\n",
    "ax.set_title('Tweets')\n",
    "ax.set_yticklabels([]) #Hide ticks\n",
    "ax.set_xticklabels([]) #Hide ticks\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tplt.annotate(word, xy=(Y[i, 0], Y[i, 1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
