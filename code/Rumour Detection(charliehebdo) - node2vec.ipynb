{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path_to_json = r'C:\\Users\\shakshi\\Desktop\\phemernrdataset\\pheme-rnr-dataset\\charliehebdo/'\n",
    "folders = [pos_json for pos_json in os.listdir(path_to_json) ]\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders[0]\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = r'C:\\Users\\shakshi\\Desktop\\phemernrdataset\\pheme-rnr-dataset\\charliehebdo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to display full text\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from json and write to csv file on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    data = []\n",
    "    if folder == 'non-rumours':\n",
    "            within_tweetid = os.path.join(base, folder)\n",
    "            #print(within_tweetid)\n",
    "            within_folders = [pos for pos in os.listdir(within_tweetid)]\n",
    "            #print(within_folders)\n",
    "            for file in within_folders:\n",
    "                within_tweetfolder = os.path.join(base, folder, file)\n",
    "                #print (within_tweetfolder)\n",
    "                src_tweet = [pos for pos in os.listdir(within_tweetfolder)]\n",
    "                #print(each_tweet)\n",
    "                if src_tweet[1] == 'source-tweet':\n",
    "                    source = os.path.join(base, folder, file, within_tweetfolder, src_tweet[1])\n",
    "                    #print(source)\n",
    "                    tweets = [pos for pos in os.listdir(source)]\n",
    "                    #print(tweets)\n",
    "                    for tweet in tweets:\n",
    "                        ids = os.path.join(base, folder, file, within_tweetfolder, src_tweet[1], tweet)\n",
    "                        if ids.endswith('.json'):\n",
    "                            data = []\n",
    "                            #print(ids)\n",
    "                            with open(ids,'r') as f:\n",
    "                                src_json = json.load(f)\n",
    "                            src_df = pd.DataFrame(src_json)\n",
    "                            temp = src_df.user.location\n",
    "                            temp1 = src_df.user.followers_count\n",
    "                            src_df = src_df[['text', 'created_at', 'id']]\n",
    "                            src_df['location'] = temp\n",
    "                            src_df['followers'] = temp1\n",
    "                            src_df = src_df.reset_index()\n",
    "                            src_df.drop(['index'], axis = 1, inplace = True)\n",
    "                            src_df = src_df.iloc[0,:]\n",
    "                            src_df = src_df.reset_index()\n",
    "                            src_df.drop(\"index\", axis = 1, inplace = True)\n",
    "                            data.append(src_df.iloc[2,0])\n",
    "                            data.append(src_df.iloc[0,0])\n",
    "                            data.append(src_df.iloc[1,0])\n",
    "                            data.append(src_df.iloc[3,0])\n",
    "                            data.append(src_df.iloc[4,0])\n",
    "                            print(\"***********************************************************************\")\n",
    "                        \n",
    "                            \n",
    "                        if src_tweet[0] == 'reactions':\n",
    "                            reactions = os.path.join(base, folder, file, within_tweetfolder, src_tweet[0])\n",
    "                            tweets_reactions = [pos for pos in os.listdir(reactions)]\n",
    "                            for tweet_reaction in tweets_reactions:\n",
    "                                mix_data = []\n",
    "                                mix_data.extend(data)\n",
    "                                ids_reactions = os.path.join(base, folder, file, within_tweetfolder, src_tweet[0], tweet_reaction)\n",
    "                                if ids_reactions.endswith('.json'):\n",
    "                                    data_reactions = []\n",
    "                                    with open(ids_reactions,'r') as f:\n",
    "                                        reactions_json = json.load(f)\n",
    "                                    reactions_df = pd.DataFrame(reactions_json)\n",
    "                                    temp = reactions_df.user.location\n",
    "                                    temp1 = reactions_df.user.followers_count\n",
    "                                    reactions_df = reactions_df[['text', 'created_at', 'id']]\n",
    "                                    reactions_df['location'] = temp\n",
    "                                    reactions_df['followers_count'] = temp1\n",
    "                                    reactions_df = reactions_df.reset_index()\n",
    "                                    reactions_df.drop(['index'], axis = 1, inplace = True)\n",
    "                                    reactions_df = reactions_df.iloc[0,:]\n",
    "                                    reactions_df = reactions_df.reset_index()\n",
    "                                    reactions_df.drop(\"index\", axis = 1, inplace = True)\n",
    "                                    data_reactions.append(reactions_df.iloc[2,0])\n",
    "                                    data_reactions.append(reactions_df.iloc[0,0])\n",
    "                                    data_reactions.append(reactions_df.iloc[1,0])\n",
    "                                    data_reactions.append(reactions_df.iloc[3,0])\n",
    "                                    data_reactions.append(reactions_df.iloc[4,0])\n",
    "                                    mix_data.extend(data_reactions)\n",
    "                                    mix_data.append(0)\n",
    "                                    #data_reactions.append(0)\n",
    "                                    with open(r'C:\\Users\\shakshi\\Desktop\\phemernrdataset\\pheme-rnr-dataset\\dump_charliehebdo.csv', 'a',newline = '', encoding = 'utf-8') as csvFile:\n",
    "                                        writer = csv.writer(csvFile)\n",
    "                                        writer.writerow(mix_data)\n",
    "                                    csvFile.close()\n",
    "\n",
    "                    \n",
    "                            \n",
    "                                \n",
    "                            \n",
    "    elif folder == 'rumours':\n",
    "        within_tweetid_r = os.path.join(base, folder)\n",
    "            #print(within_tweetid)\n",
    "        within_folders_r = [pos for pos in os.listdir(within_tweetid_r)]\n",
    "            #print(within_folders)\n",
    "        for file_r in within_folders_r:\n",
    "            within_tweetfolder_r = os.path.join(base, folder, file_r)\n",
    "                #print (within_tweetfolder)\n",
    "            src_tweet_r = [pos for pos in os.listdir(within_tweetfolder_r)]\n",
    "                #print(each_tweet)\n",
    "            if src_tweet_r[1] == 'source-tweet':\n",
    "                source_r = os.path.join(base, folder, file_r, within_tweetfolder_r, src_tweet_r[1])\n",
    "                    #print(source)\n",
    "                tweets_r = [pos for pos in os.listdir(source_r)]\n",
    "                    #print(tweets)\n",
    "                for tweet_r in tweets_r:\n",
    "                    ids_r = os.path.join(base, folder, file_r, within_tweetfolder_r, src_tweet_r[1], tweet_r)\n",
    "                    if ids_r.endswith('.json'):\n",
    "                        data = []\n",
    "                            #print(ids)\n",
    "                        with open(ids_r,'r') as f:\n",
    "                            src_json_r = json.load(f)\n",
    "                        src_df_r = pd.DataFrame(src_json_r)\n",
    "                        temp = src_df_r.user.location\n",
    "                        temp1 = src_df_r.user.followers_count\n",
    "                        src_df_r = src_df_r[['text', 'created_at', 'id']]\n",
    "                        src_df_r['location'] = temp\n",
    "                        src_df_r['followers'] = temp1\n",
    "                        src_df_r = src_df_r.reset_index()\n",
    "                        src_df_r.drop(['index'], axis = 1, inplace = True)\n",
    "                        src_df_r = src_df_r.iloc[0,:]\n",
    "                        src_df_r = src_df_r.reset_index()\n",
    "                        src_df_r.drop(\"index\", axis = 1, inplace = True)\n",
    "                        data.append(src_df_r.iloc[2,0])\n",
    "                        data.append(src_df_r.iloc[0,0])\n",
    "                        data.append(src_df_r.iloc[1,0])\n",
    "                        data.append(src_df_r.iloc[3,0])\n",
    "                        data.append(src_df_r.iloc[4,0])\n",
    "                        print(\"***********************************************************************\")\n",
    "                    if src_tweet_r[0] == 'reactions':\n",
    "                            reactions_r = os.path.join(base, folder, file_r, within_tweetfolder_r, src_tweet_r[0])\n",
    "                            tweets_reactions_r = [pos for pos in os.listdir(reactions_r)]\n",
    "                            for tweet_reaction_r in tweets_reactions_r:\n",
    "                                mix_data_r = []\n",
    "                                mix_data_r.extend(data)\n",
    "                                ids_reactions_r = os.path.join(base, folder, file_r, within_tweetfolder_r, src_tweet_r[0], tweet_reaction_r)\n",
    "                                if ids_reactions_r.endswith('.json'):\n",
    "                                    data_reactions_r = []\n",
    "                                    with open(ids_reactions_r,'r') as f:\n",
    "                                        reactions_json_r = json.load(f)\n",
    "                                    reactions_df_r = pd.DataFrame(reactions_json_r)\n",
    "                                    temp = reactions_df_r.user.location\n",
    "                                    temp1 = reactions_df_r.user.followers_count\n",
    "                                    reactions_df_r = reactions_df_r[['text', 'created_at', 'id']]\n",
    "                                    reactions_df_r['location'] = temp\n",
    "                                    reactions_df_r['followers'] = temp1\n",
    "                                    reactions_df_r = reactions_df_r.reset_index()\n",
    "                                    reactions_df_r.drop(['index'], axis = 1, inplace = True)\n",
    "                                    reactions_df_r = reactions_df_r.iloc[0,:]\n",
    "                                    reactions_df_r = reactions_df_r.reset_index()\n",
    "                                    reactions_df_r.drop(\"index\", axis = 1, inplace = True)\n",
    "                                    data_reactions_r.append(reactions_df_r.iloc[2,0])\n",
    "                                    data_reactions_r.append(reactions_df_r.iloc[0,0])\n",
    "                                    data_reactions_r.append(reactions_df_r.iloc[1,0])\n",
    "                                    data_reactions_r.append(reactions_df_r.iloc[3,0])\n",
    "                                    data_reactions_r.append(reactions_df_r.iloc[4,0])\n",
    "                                    mix_data_r.extend(data_reactions_r)\n",
    "                                    mix_data_r.append(1)\n",
    "                                    #data_reactions.append(0)\n",
    "                                    with open(r'C:\\Users\\shakshi\\Desktop\\phemernrdataset\\pheme-rnr-dataset\\dump_charliehebdo.csv', 'a',newline = '', encoding = 'utf-8') as csvFile:\n",
    "                                        writer = csv.writer(csvFile)\n",
    "                                        writer.writerow(mix_data_r)\n",
    "                                    csvFile.close()\n",
    "                    \n",
    "    del src_df\n",
    "    del src_df_r\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "print(within_tweetid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv file where data is written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\shakshi\\Desktop\\phemernrdataset\\pheme-rnr-dataset\\dump_charliehebdo.csv', encoding = 'utf-8', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['id','text', 'time', 'location','followers', 'reply_id','reply_text', 'reply_time', 'reply_location', 'reply_followers', 'rumour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rumour_df = df.sample(frac = 1)\n",
    "rumour_df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df = rumour_df.reset_index()\n",
    "rumour_df = rumour_df.drop('index', axis = 1)\n",
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# function for cleaning data\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df['clean_text'] = np.vectorize(remove_pattern)(rumour_df['text'], \"@[\\w]*\")\n",
    "rumour_df['clean_reply'] = np.vectorize(remove_pattern)(rumour_df['reply_text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df = rumour_df[['id', 'text', 'followers', 'reply_id', 'reply_text','reply_followers', 'clean_text', 'clean_reply', 'rumour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df['clean_text'] = rumour_df['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "rumour_df['clean_reply'] = rumour_df['clean_reply'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df['clean_text'] = rumour_df.clean_text.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "rumour_df['clean_reply'] = rumour_df.clean_reply.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.clean_text = rumour_df.clean_text.apply(lambda x: x.split())\n",
    "rumour_df.clean_reply = rumour_df.clean_reply.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet_text = rumour_df.clean_text.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet_reply = rumour_df.clean_reply.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet_reply.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet_text)):\n",
    "    tokenized_tweet_text[i] = ' '.join(tokenized_tweet_text[i])    \n",
    "rumour_df['clean_text'] = tokenized_tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet_reply)):\n",
    "    tokenized_tweet_reply[i] = ' '.join(tokenized_tweet_reply[i])    \n",
    "rumour_df['clean_reply'] = tokenized_tweet_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumour_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "words = ' '.join([text for text in rumour_df['clean_text']]) \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=600, height=600, random_state=129, max_font_size=120).generate(words) \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "words = ' '.join([text for text in rumour_df['clean_reply']]) \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=600, height=600, random_state=129, max_font_size=120).generate(words) \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in rumour_df['clean'][rumour_df['rumour'] == 0]]) # 0 means not a rumour \n",
    "\n",
    "wordcloud = WordCloud(width=600, height=600, random_state=129, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in rumour_df['clean'][rumour_df['rumour'] == 1]]) \n",
    "\n",
    "wordcloud = WordCloud(width=600, height=600, random_state=129, max_font_size=120).generate(normal_words)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_graph = rumour_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df=short_graph, source='id', target='reply_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(short_graph.id)\n",
    "nodes1 = list(short_graph.reply_id)\n",
    "nodes.extend(nodes1)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(nodes_for_adding=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, with_labels=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.algorithms.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G) # Average edge density of the Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(G) # Average shortest path length for ALL paths in the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(G, dimensions=20, walk_length=10, num_walks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec.walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn embeddings \n",
    "model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, _ in model.most_similar('552784898743099392'):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'node2vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(file,binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "emb = {}\n",
    "f = open(os.path.join('', 'node2vec.txt'), encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:])\n",
    "    emb[word] = value\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb['552784898743099392']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
